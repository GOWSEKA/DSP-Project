{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-10-30T05:55:43.421966Z","iopub.status.busy":"2025-10-30T05:55:43.421202Z","iopub.status.idle":"2025-10-30T05:56:36.257757Z","shell.execute_reply":"2025-10-30T05:56:36.256738Z","shell.execute_reply.started":"2025-10-30T05:55:43.421929Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-30T06:05:41.626551Z","iopub.status.busy":"2025-10-30T06:05:41.626117Z","iopub.status.idle":"2025-10-30T06:05:41.631721Z","shell.execute_reply":"2025-10-30T06:05:41.630732Z","shell.execute_reply.started":"2025-10-30T06:05:41.626524Z"},"trusted":true},"outputs":[],"source":["import os\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-30T06:05:50.282521Z","iopub.status.busy":"2025-10-30T06:05:50.282158Z","iopub.status.idle":"2025-10-30T06:05:50.287476Z","shell.execute_reply":"2025-10-30T06:05:50.286591Z","shell.execute_reply.started":"2025-10-30T06:05:50.282491Z"},"trusted":true},"outputs":[],"source":["import pathlib\n","\n","dataset_path = \"/kaggle/input/food-freshness-dataset/Dataset\"\n","dataset_path = pathlib.Path(dataset_path)  # <-- convert string to Path\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-30T06:05:56.425771Z","iopub.status.busy":"2025-10-30T06:05:56.424716Z","iopub.status.idle":"2025-10-30T06:06:45.677156Z","shell.execute_reply":"2025-10-30T06:06:45.675779Z","shell.execute_reply.started":"2025-10-30T06:05:56.425721Z"},"trusted":true},"outputs":[],"source":["# Filter only valid image extensions\n","valid_extensions = [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\"]\n","all_image_paths = [str(p) for p in dataset_path.rglob(\"*\") if p.suffix.lower() in valid_extensions]\n","\n","print(f\"Total valid images: {len(all_image_paths)}\")\n","\n","# Check for corrupted images\n","for img_path in all_image_paths:\n","    try:\n","        img = Image.open(img_path)\n","        img.verify()  # verify that it is not corrupted\n","    except Exception as e:\n","        print(f\"Corrupted image: {img_path}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-30T06:30:24.869148Z","iopub.status.busy":"2025-10-30T06:30:24.864969Z","iopub.status.idle":"2025-10-30T06:31:34.067722Z","shell.execute_reply":"2025-10-30T06:31:34.066512Z","shell.execute_reply.started":"2025-10-30T06:30:24.868999Z"},"trusted":true},"outputs":[],"source":["train_ds_raw = tf.keras.utils.image_dataset_from_directory(\n","    dataset_path,\n","    validation_split=0.3,\n","    subset=\"training\",\n","    seed=123,\n","    image_size=(128, 128),\n","    batch_size=32\n",")\n","\n","val_ds_raw = tf.keras.utils.image_dataset_from_directory(\n","    dataset_path,\n","    validation_split=0.3,\n","    subset=\"validation\",\n","    seed=123,\n","    image_size=(128, 128),\n","    batch_size=32\n",")\n","\n","print(\"Class names:\", train_ds_raw.class_names)  # get classes before mapping\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-30T06:35:01.338891Z","iopub.status.busy":"2025-10-30T06:35:01.338584Z","iopub.status.idle":"2025-10-30T06:35:01.384897Z","shell.execute_reply":"2025-10-30T06:35:01.38412Z","shell.execute_reply.started":"2025-10-30T06:35:01.338872Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras.layers import Rescaling\n","\n","normalization_layer = Rescaling(1./255)\n","\n","train_ds = train_ds_raw.map(lambda x, y: (normalization_layer(x), y))\n","train_ds = train_ds.apply(tf.data.experimental.ignore_errors())  # skip bad images\n","\n","val_ds = val_ds_raw.map(lambda x, y: (normalization_layer(x), y))\n","val_ds = val_ds.apply(tf.data.experimental.ignore_errors())      # skip bad images\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-30T06:35:07.98306Z","iopub.status.busy":"2025-10-30T06:35:07.982726Z","iopub.status.idle":"2025-10-30T06:35:08.065435Z","shell.execute_reply":"2025-10-30T06:35:08.064588Z","shell.execute_reply.started":"2025-10-30T06:35:07.983028Z"},"trusted":true},"outputs":[],"source":["num_classes = len(train_ds_raw.class_names)\n","\n","model = keras.Sequential([\n","    layers.Conv2D(32, (3,3), activation='relu', input_shape=(128,128,3)),\n","    layers.MaxPooling2D(),\n","    layers.Conv2D(64, (3,3), activation='relu'),\n","    layers.MaxPooling2D(),\n","    layers.Flatten(),\n","    layers.Dense(64, activation='relu'),\n","    layers.Dense(num_classes, activation='softmax')\n","])\n","\n","model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-30T06:35:12.163194Z","iopub.status.busy":"2025-10-30T06:35:12.162865Z","iopub.status.idle":"2025-10-30T06:36:59.996057Z","shell.execute_reply":"2025-10-30T06:36:59.995086Z","shell.execute_reply.started":"2025-10-30T06:35:12.163169Z"},"trusted":true},"outputs":[],"source":["val_loss, val_acc = model.evaluate(val_ds)\n","print(f\"Validation Accuracy: {val_acc:.2f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-30T06:37:55.805195Z","iopub.status.busy":"2025-10-30T06:37:55.804837Z","iopub.status.idle":"2025-10-30T07:19:10.791178Z","shell.execute_reply":"2025-10-30T07:19:10.782654Z","shell.execute_reply.started":"2025-10-30T06:37:55.805173Z"},"trusted":true},"outputs":[],"source":["history = model.fit(\n","    train_ds,\n","    validation_data=val_ds,\n","    epochs=3\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-30T07:19:46.218556Z","iopub.status.busy":"2025-10-30T07:19:46.218053Z","iopub.status.idle":"2025-10-30T07:19:47.123844Z","shell.execute_reply":"2025-10-30T07:19:47.122647Z","shell.execute_reply.started":"2025-10-30T07:19:46.21848Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","plt.plot(history.history['accuracy'], label='train_acc')\n","plt.plot(history.history['val_accuracy'], label='val_acc')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-30T07:25:12.298939Z","iopub.status.busy":"2025-10-30T07:25:12.298533Z","iopub.status.idle":"2025-10-30T07:25:12.304895Z","shell.execute_reply":"2025-10-30T07:25:12.303678Z","shell.execute_reply.started":"2025-10-30T07:25:12.298913Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.preprocessing import image\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-30T07:25:15.338223Z","iopub.status.busy":"2025-10-30T07:25:15.337926Z","iopub.status.idle":"2025-10-30T07:25:15.343286Z","shell.execute_reply":"2025-10-30T07:25:15.34202Z","shell.execute_reply.started":"2025-10-30T07:25:15.338203Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.preprocessing import image\n","import numpy as np\n","import matplotlib.pyplot as plt\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-30T07:26:29.341401Z","iopub.status.busy":"2025-10-30T07:26:29.340996Z","iopub.status.idle":"2025-10-30T07:26:29.347449Z","shell.execute_reply":"2025-10-30T07:26:29.345696Z","shell.execute_reply.started":"2025-10-30T07:26:29.341352Z"},"trusted":true},"outputs":[],"source":["img_path = \"/kaggle/input/fruits/fruits.jpg\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-30T07:26:32.866367Z","iopub.status.busy":"2025-10-30T07:26:32.866029Z","iopub.status.idle":"2025-10-30T07:26:32.872428Z","shell.execute_reply":"2025-10-30T07:26:32.871431Z","shell.execute_reply.started":"2025-10-30T07:26:32.866342Z"},"trusted":true},"outputs":[],"source":["def preprocess_image(img_path):\n","    img = image.load_img(img_path, target_size=(128, 128))  # same size as training\n","    img_array = image.img_to_array(img)\n","    img_array = img_array / 255.0   # normalize to [0,1]\n","    img_array = np.expand_dims(img_array, axis=0)  # add batch dimension\n","    return img_array"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-30T07:26:36.121174Z","iopub.status.busy":"2025-10-30T07:26:36.119977Z","iopub.status.idle":"2025-10-30T07:26:36.170251Z","shell.execute_reply":"2025-10-30T07:26:36.16927Z","shell.execute_reply.started":"2025-10-30T07:26:36.121139Z"},"trusted":true},"outputs":[],"source":["img = preprocess_image(img_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-30T07:26:47.689877Z","iopub.status.busy":"2025-10-30T07:26:47.689436Z","iopub.status.idle":"2025-10-30T07:26:47.909543Z","shell.execute_reply":"2025-10-30T07:26:47.908689Z","shell.execute_reply.started":"2025-10-30T07:26:47.689852Z"},"trusted":true},"outputs":[],"source":["pred = model.predict(img)\n","pred_class = np.argmax(pred, axis=1)[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-30T07:27:00.490064Z","iopub.status.busy":"2025-10-30T07:27:00.489247Z","iopub.status.idle":"2025-10-30T07:27:00.494762Z","shell.execute_reply":"2025-10-30T07:27:00.493678Z","shell.execute_reply.started":"2025-10-30T07:27:00.490034Z"},"trusted":true},"outputs":[],"source":["class_names = train_ds_raw.class_names  # ['Fresh', 'Rotten']\n","predicted_label = class_names[pred_class]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-30T07:27:09.899964Z","iopub.status.busy":"2025-10-30T07:27:09.899645Z","iopub.status.idle":"2025-10-30T07:27:10.273115Z","shell.execute_reply":"2025-10-30T07:27:10.271805Z","shell.execute_reply.started":"2025-10-30T07:27:09.899941Z"},"trusted":true},"outputs":[],"source":["print(f\"Predicted class: {predicted_label}\")\n","\n","plt.imshow(image.load_img(img_path))\n","plt.title(f\"Prediction: {predicted_label}\")\n","plt.axis(\"off\")\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-30T07:33:13.050529Z","iopub.status.busy":"2025-10-30T07:33:13.050082Z","iopub.status.idle":"2025-10-30T07:33:13.392414Z","shell.execute_reply":"2025-10-30T07:33:13.391432Z","shell.execute_reply.started":"2025-10-30T07:33:13.050499Z"},"trusted":true},"outputs":[],"source":["import os\n","from tensorflow.keras.preprocessing import image\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Folder path\n","test_folder = \"/kaggle/input/fruits/\"\n","\n","# Collect all valid image paths\n","valid_extensions = [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\"]\n","test_image_paths = [os.path.join(test_folder, f) for f in os.listdir(test_folder)\n","                    if os.path.splitext(f)[1].lower() in valid_extensions]\n","\n","# Initialize counters for each class\n","class_names = train_ds_raw.class_names  # ['Fresh', 'Rotten']\n","class_counts = {cls: 0 for cls in class_names}\n","\n","# Predict each image\n","for img_path in test_image_paths:\n","    img = image.load_img(img_path, target_size=(128, 128))\n","    img_array = image.img_to_array(img) / 255.0\n","    img_array = np.expand_dims(img_array, axis=0)\n","\n","    pred = model.predict(img_array)\n","    pred_class = np.argmax(pred, axis=1)[0]\n","    class_label = class_names[pred_class]\n","    class_counts[class_label] += 1\n","\n","    print(f\"{os.path.basename(img_path)} âžœ {class_label}\")\n","\n","# Summary\n","print(\"\\n=== Summary ===\")\n","total_images = len(test_image_paths)\n","for cls, count in class_counts.items():\n","    print(f\"{cls}: {count}\")\n","print(f\"Total images: {total_images}\")\n","\n","# Visualization\n","plt.bar(class_counts.keys(), class_counts.values(), color=[\"green\", \"red\"])\n","plt.title(\"Fresh vs Rotten Fruit Count\")\n","plt.ylabel(\"Number of Images\")\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-30T07:42:10.117939Z","iopub.status.busy":"2025-10-30T07:42:10.117484Z","iopub.status.idle":"2025-10-30T07:42:10.579995Z","shell.execute_reply":"2025-10-30T07:42:10.578947Z","shell.execute_reply.started":"2025-10-30T07:42:10.117911Z"},"trusted":true},"outputs":[],"source":["import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Load image\n","img_path = \"/kaggle/input/fruits/fruits.jpg\"\n","image = cv2.imread(img_path)\n","image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","plt.imshow(image)\n","plt.axis(\"off\")\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-30T07:42:23.190918Z","iopub.status.busy":"2025-10-30T07:42:23.190548Z","iopub.status.idle":"2025-10-30T07:42:23.500416Z","shell.execute_reply":"2025-10-30T07:42:23.499259Z","shell.execute_reply.started":"2025-10-30T07:42:23.190892Z"},"trusted":true},"outputs":[],"source":["# Convert to HSV\n","hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n","\n","# Define a mask to extract colored areas (avoid background)\n","mask = cv2.inRange(hsv, (0, 40, 40), (179, 255, 255))\n","result = cv2.bitwise_and(image, image, mask=mask)\n","\n","plt.imshow(result)\n","plt.axis(\"off\")\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-30T08:57:35.834932Z","iopub.status.busy":"2025-10-30T08:57:35.834656Z","iopub.status.idle":"2025-10-30T08:57:36.959253Z","shell.execute_reply":"2025-10-30T08:57:36.958242Z","shell.execute_reply.started":"2025-10-30T08:57:35.834898Z"},"trusted":true},"outputs":[],"source":["import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Load image\n","image = cv2.imread(\"/kaggle/input/fruits/fruits.jpg\")\n","image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","# Convert to HSV color space (better for color-based segmentation)\n","hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n","\n","# Define color ranges for typical fruit colors (tune if needed)\n","color_ranges = {\n","    \"red\": [(0, 80, 80), (10, 255, 255)],       # strawberries, apples\n","    \"yellow\": [(20, 80, 80), (35, 255, 255)],   # bananas\n","    \"orange\": [(10, 100, 100), (25, 255, 255)], # oranges\n","    \"green\": [(35, 60, 60), (85, 255, 255)],    # apples, unripe fruits\n","}\n","\n","# Create one combined mask for all colors\n","mask_total = np.zeros(hsv.shape[:2], dtype=np.uint8)\n","for color, (lower, upper) in color_ranges.items():\n","    lower = np.array(lower)\n","    upper = np.array(upper)\n","    mask = cv2.inRange(hsv, lower, upper)\n","    mask_total = cv2.bitwise_or(mask_total, mask)\n","\n","# Morphological operations to separate touching fruits\n","kernel = np.ones((7, 7), np.uint8)\n","mask_clean = cv2.morphologyEx(mask_total, cv2.MORPH_OPEN, kernel)\n","mask_clean = cv2.morphologyEx(mask_clean, cv2.MORPH_CLOSE, kernel)\n","\n","# Find contours\n","contours, _ = cv2.findContours(mask_clean, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","\n","objects = []\n","for cnt in contours:\n","    x, y, w, h = cv2.boundingRect(cnt)\n","    if w > 40 and h > 40:  # ignore small noise\n","        objects.append((x, y, w, h))\n","        cv2.rectangle(image, (x, y), (x+w, y+h), (255, 0, 0), 3)\n","\n","# Show results\n","plt.figure(figsize=(10, 8))\n","plt.imshow(image)\n","plt.axis(\"off\")\n","plt.title(f\"Detected {len(objects)} possible fruits\")\n","plt.show()\n","\n","print(f\"Detected {len(objects)} possible fruits.\")\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":7054921,"sourceId":11283779,"sourceType":"datasetVersion"},{"datasetId":8324729,"sourceId":13139852,"sourceType":"datasetVersion"},{"datasetId":8607913,"sourceId":13552928,"sourceType":"datasetVersion"}],"dockerImageVersionId":31089,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"}},"nbformat":4,"nbformat_minor":4}
